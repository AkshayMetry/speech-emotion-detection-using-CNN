
# Speech Emotion Detection

Speech emotion detection is the process of analyzing the acoustic features of speech to identify the underlying emotions conveyed in the speech signal. 

CNNs (Convolutional Neural Networks) are a popular deep learning model used in speech emotion detection due to their ability to extract relevant features from the input signal.

Data augmentation is a technique used to increase the amount of training data available to the model by creating synthetic data from the existing data. This can help to improve the robustness of the model and prevent overfitting.

In speech emotion detection using CNNs and data augmentation, the input speech signal is first preprocessed to extract relevant acoustic features such as MFCCs (Mel-frequency cepstral coefficients) and mel-spectrum features. The CNN model is then trained using the preprocessed data and augmented data to improve its ability to generalize to new, unseen data.

Accuracy achieved in this project is 84%

## Tech Stack

**Programming Language** : Python

**Libraries** : Tensorflow, Seaborn, Scikit Learn, Matplotlib


## Datasets used in this project


Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D) - https://www.kaggle.com/datasets/ejlok1/cremad

Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess) - https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio

Surrey Audio-Visual Expressed Emotion (Savee) - https://www.kaggle.com/datasets/barelydedicated/savee-database

Toronto emotional speech set (Tess) - https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess


## Results

[https://github.com/AkshayMetry/speech-emotion-detection-using-cnn/img.png](https://github.com/AkshayMetry/speech-emotion-detection-using-cnn/blob/main/img.png)
